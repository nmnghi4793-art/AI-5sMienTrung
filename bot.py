# bot.py
# Bot 5S ho√†n ch·ªânh + G·ªòP TIN NH·∫ÆN X√ÅC NH·∫¨N (kh√¥ng c√≤n c√¢u "C√≤n thi·∫øu X ·∫£nh")
# ---------------------------------------------------------------------------------
# T√≠nh nƒÉng:
# - Nh·∫≠n ·∫£nh theo ID kho (kh√¥ng c·∫ßn tag). C√≥ th·ªÉ g·ª≠i 1 text ch·ª©a ID/Ng√†y tr∆∞·ªõc r·ªìi g·ª≠i nhi·ªÅu ·∫£nh li·ªÅn sau (kh√¥ng caption).
# - Gh√©p caption trong 2 ph√∫t (text tr∆∞·ªõc √°p cho ·∫£nh sau).
# - Ki·ªÉm tra tr√πng ·∫£nh: trong c√πng l√¥ g·ª≠i (album), tr√πng trong ng√†y theo kho, v√† tr√πng v·ªõi l·ªãch s·ª≠ (log l√† "·∫£nh qu√° kh·ª©").
# - ƒê·∫øm s·ªë ·∫£nh m·ªói kho/ng√†y; ph·∫£n h·ªìi **g·ªôp** v√†o 1 tin duy nh·∫•t cho m·ªói (kho, ng√†y) v√† update ti·∫øn ƒë·ªô 1/4 ‚Üí 2/4 ‚Üí 3/4 ‚Üí 4/4.
# - B√°o c√°o 21:00 theo format b·∫°n y√™u c·∫ßu:
#   (1) C√°c kho ch∆∞a b√°o c√°o 5S ‚Üí "- `ID` - T√™n kho"
#   (2) Kho s·ª≠ d·ª•ng ·∫£nh c≈©/qu√° kh·ª© ‚Üí "- `ID`: tr√πng ·∫£nh ng√†y dd/mm/yyyy" ho·∫∑c "Kh√¥ng c√≥"
#   (3) Ch·ªâ li·ªát k√™ kho CH∆ØA ƒê·ª¶; n·∫øu kh√¥ng c√≥ th√¨ "T·∫•t c·∫£ kho ƒë√£ g·ª≠i ƒë·ªß s·ªë l∆∞·ª£ng ·∫£nh theo quy ƒë·ªãnh"
# - L·ªánh: /chatid (xem chat id), /report_now (g·ª≠i b√°o c√°o ngay).
#
# C·∫•u h√¨nh ENV b·∫Øt bu·ªôc: BOT_TOKEN
# Tu·ª≥ ch·ªçn ENV: REPORT_CHAT_IDS="-100111,-100222", REQUIRED_PHOTOS="4"
# Excel b·∫Øt bu·ªôc: danh_sach_nv_theo_id_kho.xlsx c√≥ c·ªôt: id_kho, ten_kho

import os
import re
import json
import hashlib
from datetime import datetime, date, time as dtime
from zoneinfo import ZoneInfo

import pandas as pd
from telegram import Update
from telegram.constants import ParseMode
from telegram.ext import (
    ApplicationBuilder, Application, CommandHandler, MessageHandler,
    ContextTypes, filters
)

# ===== Scoring imports & ENV =====
import cv2
import numpy as np
import random, time

SCORING_ENABLED = os.getenv("SCORING_ENABLED","0") == "1"
SCORING_MODE = os.getenv("SCORING_MODE","rule").strip().lower() or "rule"
# Default weights per area
_DEFAULT_WEIGHTS = {
    "HangHoa": {"align": 40, "tidy": 40, "aisle": 20},
    "WC":      {"stain": 60, "trash": 25, "dry": 15},
    "KhoBai":  {"clean": 50, "obstacle": 30, "line": 20},
    "VanPhong":{"desk_tidy": 45, "surface_clean": 35, "cable": 20}

}
# Ng∆∞·ª°ng ƒë·ªÉ n√™u v·∫•n ƒë·ªÅ/khuy·∫øn ngh·ªã (0..1)
_AREA_RULE_THRESHOLDS = {
    "HangHoa": {"align": 0.70, "tidy": 0.60, "aisle": 0.70},
    "WC":      {"stain": 0.80, "trash": 0.80, "dry": 0.70},
    "KhoBai":  {"clean": 0.80, "obstacle": 0.75, "line": 0.70},
    "VanPhong":{"desk_tidy": 0.75, "surface_clean": 0.80, "cable": 0.70}
}
try:
    AREA_RULE_THRESHOLDS = json.loads(os.getenv("AREA_RULE_THRESHOLDS","") or "{}")
    for k, v in _AREA_RULE_THRESHOLDS.items():
        AREA_RULE_THRESHOLDS.setdefault(k, v)
except Exception:
    AREA_RULE_THRESHOLDS = _AREA_RULE_THRESHOLDS
try:
    AREA_RULE_WEIGHTS = json.loads(os.getenv("AREA_RULE_WEIGHTS","") or "{}")
    if not AREA_RULE_WEIGHTS:
        AREA_RULE_WEIGHTS = _DEFAULT_WEIGHTS
except Exception:
    AREA_RULE_WEIGHTS = _DEFAULT_WEIGHTS

# Parameters for quality scoring
BRIGHT_MIN, BRIGHT_MAX = 70, 200
MIN_SHORT_EDGE = 600
LAPLACIAN_GOOD = 250.0

# Regex to detect KV in caption/text, e.g., "KV: HangHoa" or "Khu_vuc=WC"
AREA_RX = re.compile(r'(?:\bkv\b|\bkhu[_\s]*vuc)\s*[:=]\s*([a-zA-Z0-9_-]{2,})', re.I)


# ========= C·∫§U H√åNH =========
EXCEL_PATH = "danh_sach_nv_theo_id_kho.xlsx"  # Excel: c·ªôt id_kho, ten_kho
HASH_DB_PATH = "hashes.json"                  # l∆∞u hash ·∫£nh (ph√°t hi·ªán tr√πng)
SUBMIT_DB_PATH = "submissions.json"           # { "YYYY-MM-DD": ["id1","id2",...] }
COUNT_DB_PATH = "counts.json"                 # { "YYYY-MM-DD": { "id_kho": count } }
PAST_DB_PATH  = "past_uses.json"              # { "YYYY-MM-DD": [ {id_kho, prev_date, hash} ] }
TZ = ZoneInfo("Asia/Ho_Chi_Minh")             # m√∫i gi·ªù VN
REPORT_HOUR = 21                              # 21:00 h·∫±ng ng√†y
TEXT_PAIR_TIMEOUT = 120                       # gi√¢y: gi·ªØ caption/text d√πng chung
REQUIRED_PHOTOS = int(os.getenv("REQUIRED_PHOTOS", "4"))
# C·ªë ƒë·ªãnh group nh·∫≠n b√°o c√°o (c√≥ th·ªÉ override b·∫±ng ENV REPORT_CHAT_IDS)
DEFAULT_REPORT_CHAT_IDS = [-1002688907477]


# ========= C·∫¢NH B√ÅO TR·ªÑ (6s) =========
# L∆∞u job c·∫£nh b√°o theo (chat_id, id_kho, day_key) ƒë·ªÉ tr√°nh spam khi g·ª≠i li√™n ti·∫øp
WARN_JOBS = {}  # {(chat_id, id_kho, day_key): Job}

# ========= SCORING: helper to compact message & delayed send (5s) =========
def compact_scoring_text(full_md: str) -> str:
    lines = [ln for ln in (full_md or '').splitlines() if ln.strip()]
    kept = []
    for ln in lines:
        st = ln.strip()
        if st.startswith("- KV:"):
            continue
        if st.startswith("- Ch·∫•t l∆∞·ª£ng:"):
            continue
        kept.append(ln)
    return "\n".join(kept)

async def _send_scoring_job(context: ContextTypes.DEFAULT_TYPE):
    data = context.job.data or {}
    chat_id = data.get("chat_id")
    text = data.get("text")
    if chat_id and text:
        try:
            await context.bot.send_message(chat_id=chat_id, text=text, parse_mode=ParseMode.MARKDOWN)
        except Exception:
            pass


def _day_key(d):
    return d.isoformat()


# ==== G·ªòP TIN NH·∫ÆN CH·∫§M ƒêI·ªÇM (AGGREGATE) ====
from collections import defaultdict

# ---- Duplicate similarity tracking (pHash) ----
def _phash_cv(img_bgr):
    import cv2, numpy as np
    gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)
    small = cv2.resize(gray, (32, 32), interpolation=cv2.INTER_AREA)
    dct = cv2.dct(np.float32(small))
    block = dct[:8, :8]
    med = np.median(block[1:])
    bits = (block > med).flatten()
    h = 0
    for b in bits:
        h = (h << 1) | int(bool(b))
    return int(h) & ((1<<64)-1)

def _hamming64(a: int, b: int) -> int:
    return int(bin((a ^ b) & ((1<<64)-1)).count("1"))

from collections import deque
DUP_HISTORY = {}  # key: f"{chat_id}|{id_kho}" -> deque[(phash:int, ngay_str:str)]
_DUP_MAX = 60

def _dup_key(chat_id: int, id_kho: str) -> str:
    return f"{chat_id}|{id_kho}"

def _dup_best_match(dup_key: str, phash: int):
    dq = DUP_HISTORY.get(dup_key, deque())
    best_sim, best_date = 0.0, None
    for h, dstr in dq:
        ham = _hamming64(h, phash)
        sim = 1.0 - (ham / 64.0)
        if sim > best_sim:
            best_sim, best_date = sim, dstr
    return best_sim, best_date

def _dup_push(dup_key: str, phash: int, ngay_str: str):
    dq = DUP_HISTORY.setdefault(dup_key, deque(maxlen=_DUP_MAX))
    dq.append((phash, ngay_str))
SCORING_BUFFER = defaultdict(list)  # key -> list[dict]
SCORING_JOBS = {}

def _scoring_key(chat_id: int, id_kho: str, ngay_str: str) -> str:
    return f"{chat_id}|{id_kho}|{ngay_str}"



# ============ DIAGNOSTICS VARIETY ============

# Kho c√¢u theo t·ª´ng KV & h·∫°ng m·ª•c
SIMPLE_ISSUE_BANK = {
    "HangHoa": {
        "align": [
            "H√†ng ch∆∞a th·∫≥ng h√†ng, l·ªách so v·ªõi m√©p k·ªá",
            "C√°c ki·ªán/th√πng x·∫øp kh√¥ng song song, t·∫°o c·∫£m gi√°c l·ªôn x·ªôn",
            "M·ªôt s·ªë pallet b·ªã xoay kh√°c h∆∞·ªõng c√≤n l·∫°i",
            "C√≥ kho·∫£ng h·ªü/nh√¥ ra ·ªü d√£y x·∫øp g√¢y m·∫•t th·∫©m m·ªπ"
        ],
        "tidy": [
            "Khu v·ª±c b·ªÅ b·ªôn, nhi·ªÅu v·∫≠t nh·ªè r·ªùi r·∫°c",
            "Th√πng r·ªóng/chai l·ªç ch∆∞a gom v·ªÅ khu t·∫≠p k·∫øt",
            "V·∫≠t d·ª•ng t·∫°m ƒë·∫∑t sai khu v·ª±c quy ƒë·ªãnh",
            "C√≥ r√°c v·ª•n/bao b√¨ th·ª´a tr√™n b·ªÅ m·∫∑t"
        ],
        "aisle": [
            "L·ªëi ƒëi b·ªã h·∫πp, c√≥ v·∫≠t c·∫£n l·∫•n line",
            "Kho·∫£ng di chuy·ªÉn ch∆∞a th√¥ng tho√°ng",
            "Pallet/ki·ªán h√†ng ƒë·∫∑t s√°t ho·∫∑c ƒë√® l√™n v·∫°ch k·∫ª",
            "H√†nh lang kh√¥ng ƒë·∫£m b·∫£o an to√†n khi l∆∞u th√¥ng"
        ]
    },
    "WC": {
        "stain": [
            "B·ªìn/s√†n c√≥ v·ªát ·ªë ho·∫∑c b√°m b·∫©n th·∫•y r√µ",
            "V√°ch/thi·∫øt b·ªã v·ªá sinh c√≤n v·ªát n∆∞·ªõc v√† c·∫∑n",
            "G√≥c c·∫°nh/kh√≥ v·ªá sinh c√≤n b√°m d∆°"
        ],
        "trash": [
            "S√†n c√≤n r√°c/gi·∫•y v·ª•n",
            "Th√πng r√°c ƒë·∫ßy ho·∫∑c kh√¥ng c√≥ n·∫Øp",
            "M·ªôt s·ªë khu v·ª±c thi·∫øu ƒëi·ªÉm t·∫≠p k·∫øt r√°c"
        ],
        "dry": [
            "S√†n ∆∞·ªõt, c√≥ nguy c∆° tr∆°n tr∆∞·ª£t",
            "Ch∆∞a lau kh√¥ sau khi v·ªá sinh",
            "Thi·∫øu bi·ªÉn c·∫£nh b√°o khu v·ª±c s√†n ∆∞·ªõt"
        ]
    },
    "KhoBai": {
        "clean": [
            "S√†n kho c√≤n b·ª•i b·∫©n/m·∫£ng t·ªëi",
            "D·∫ßu m·ª°/ƒë·∫•t c√°t ch∆∞a x·ª≠ l√Ω tri·ªát ƒë·ªÉ",
            "Khu v·ª±c t·∫£i/x·∫øp d·ª° b√°m d∆°"
        ],
        "obstacle": [
            "L·ªëi ƒëi c√≥ v·∫≠t c·∫£n/ƒë·∫∑t ƒë·ªì t·∫°m",
            "H√†ng t·∫°m th·ªùi ch∆∞a quy ho·∫°ch, che khu line",
            "Ch∆∞·ªõng ng·∫°i l√†m c·∫£n tr·ªü xe n√¢ng/ng∆∞·ªùi ƒëi b·ªô"
        ],
        "line": [
            "V·∫°ch k·∫ª ch·ªâ d·∫´n m·ªù/kh√≥ nh√¨n",
            "Bi·ªÉn b√°o v·ªã tr√≠ ch∆∞a n·ªïi b·∫≠t",
            "Thi·∫øu nh√£n/v·∫°ch k·∫ª t·∫°i m·ªôt s·ªë √¥/khu"
        ]
    },
    "VanPhong": {
        "desk_tidy": [
            "B√†n l√†m vi·ªác b·ª´a b·ªôn, nhi·ªÅu v·∫≠t ch∆∞a ph√¢n lo·∫°i",
            "T√†i li·ªáu/ƒë·ªì c√° nh√¢n ch∆∞a ƒë·ªÉ ƒë√∫ng v·ªã tr√≠",
            "Thi·∫øu khay/h·ªôp gi√∫p s·∫Øp x·∫øp g·ªçn"
        ],
        "surface_clean": [
            "B·ªÅ m·∫∑t c√≥ b·ª•i/v·ªát b·∫©n",
            "M√†n h√¨nh/thi·∫øt b·ªã c√≥ d·∫•u tay/b√°m b·∫©n",
            "KhƒÉn lau/dung d·ªãch v·ªá sinh ch∆∞a s·ª≠ d·ª•ng th∆∞·ªùng xuy√™n"
        ],
        "cable": [
            "D√¢y ƒëi·ªán/c√°p ch∆∞a gom g·ªçn",
            "D√¢y th·∫£ t·ª± do g√¢y r·ªëi m·∫Øt/kh√≥ d·ªçn",
            "Thi·∫øu k·∫πp/·ªëng b·ªçc ƒë·ªÉ c·ªë ƒë·ªãnh d√¢y"
        ]
    }
}

SIMPLE_REC_BANK = {
    "HangHoa": {
        "align": [
            "CƒÉn th·∫≥ng theo m√©p k·ªá ho·∫∑c v·∫°ch; xoay c√πng m·ªôt h∆∞·ªõng",
            "D√πng n√™m/ch·∫∑n m√©p ƒë·ªÉ gi·ªØ th·∫≥ng h√†ng",
            "R√† so√°t pallet l·ªách, ƒëi·ªÅu ch·ªânh l·∫°i ngay"
        ],
        "tidy": [
            "Gom th√πng r·ªóng v·ªÅ khu t·∫≠p k·∫øt; b·ªè v·∫≠t c·∫£n",
            "Ph√¢n lo·∫°i theo SKU/khu v·ª±c, d√°n nh√£n r√µ",
            "Thi·∫øt l·∫≠p th√πng/k·ªá t·∫°m cho v·∫≠t nh·ªè d·ªÖ r∆°i"
        ],
        "aisle": [
            "Gi·ªØ l·ªëi ƒëi ‚â• 1m, kh√¥ng l·∫•n v·∫°ch",
            "Di d·ªùi v·∫≠t c·∫£n kh·ªèi line; b·ªë tr√≠ khu ƒë·ªì t·∫°m ri√™ng",
            "Nh·∫Øc nh·ªü ca l√†m vi·ªác kh√¥ng x·∫øp ch·∫∑n l·ªëi ƒëi"
        ]
    },
    "WC": {
        "stain": [
            "C·ªç r·ª≠a ƒë·ªãnh k·ª≥; d√πng dung d·ªãch t·∫©y ph√π h·ª£p",
            "T·∫≠p trung v·ªá sinh g√≥c khu·∫•t/v·∫øt ·ªë kh√≥ x·ª≠ l√Ω",
            "Thi·∫øt l·∫≠p checklist v·ªá sinh theo ca"
        ],
        "trash": [
            "Thu gom r√°c ngay; d√πng th√πng r√°c c√≥ n·∫Øp",
            "B·ªë tr√≠ th√™m th√πng r√°c ·ªü ƒëi·ªÉm ph√°t sinh",
            "Nh·∫Øc ƒë·ªï r√°c cu·ªëi ca ƒë·ªÉ tr√°nh t·ªìn ƒë·ªçng"
        ],
        "dry": [
            "Lau kh√¥ s√†n sau v·ªá sinh",
            "ƒê·∫∑t bi·ªÉn c·∫£nh b√°o khi s√†n ∆∞·ªõt",
            "Ki·ªÉm tra r√≤ r·ªâ ƒë·ªÉ x·ª≠ l√Ω ngu·ªìn n∆∞·ªõc"
        ]
    },
    "KhoBai": {
        "clean": [
            "Qu√©t/lau s√†n theo l·ªãch; x·ª≠ l√Ω d·∫ßu tr√†n ngay",
            "D·ª•ng c·ª• v·ªá sinh ƒë·∫∑t s·∫µn t·∫°i khu thao t√°c",
            "√Åp d·ª•ng 5S cu·ªëi ca t·∫°i khu x·∫øp d·ª°"
        ],
        "obstacle": [
            "Quy ƒë·ªãnh khu ƒë·ªì t·∫°m, kh√¥ng ƒë·∫∑t tr√™n line",
            "L·∫≠p s∆° ƒë·ªì l·ªëi ƒëi & nh·∫Øc nh·ªü tu√¢n th·ªß",
            "D·ªçn ch∆∞·ªõng ng·∫°i ƒë·ªÉ xe n√¢ng l∆∞u th√¥ng an to√†n"
        ],
        "line": [
            "S∆°n/k·∫ª l·∫°i v·∫°ch, b·ªï sung bi·ªÉn b√°o",
            "Chu·∫©n h√≥a nh√£n v·ªã tr√≠ t·∫°i √¥ k·ªá",
            "Ki·ªÉm tra ƒë·ªãnh k·ª≥ ƒë·ªô r√µ c·ªßa line"
        ]
    },
    "VanPhong": {
        "desk_tidy": [
            "D√πng khay/h·ªôp ph√¢n lo·∫°i; d·ªçn b√†n cu·ªëi ng√†y",
            "Thi·∫øt l·∫≠p quy t·∫Øc 1 ph√∫t d·ªçn b√†n gi·ªØa ca",
            "C·∫•t ƒë·ªì c√° nh√¢n v√†o ngƒÉn/locker"
        ],
        "surface_clean": [
            "Lau b·ªÅ m·∫∑t v·ªõi dung d·ªãch ph√π h·ª£p",
            "L·∫≠p t·∫ßn su·∫•t v·ªá sinh h√†ng ng√†y/tu·∫ßn",
            "Chu·∫©n b·ªã khƒÉn lau/gi·∫•y t·∫°i ch·ªó"
        ],
        "cable": [
            "Gom d√¢y v·ªÅ m·ªôt m√©p b√†n, d√πng k·∫πp/·ªëng b·ªçc",
            "D√°n nh√£n ƒë·∫ßu d√¢y ƒë·ªÉ d·ªÖ qu·∫£n l√Ω",
            "C·ªë ƒë·ªãnh ·ªï c·∫Øm/d√¢y ngu·ªìn ƒë·ªÉ g·ªçn m·∫Øt"
        ]
    }
}

def _pick_many(pool: list, k: int = 2) -> list:
    if not pool: return []
    k = min(k, len(pool))
    return random.sample(pool, k)

def _kv_for_variety(kv_key: str) -> str:
    return kv_key if kv_key in SIMPLE_ISSUE_BANK else 'HangHoa'

def _diagnose_varied(kv_key: str, parts: dict) -> tuple[list, list]:
    """
    Sinh 'V·∫•n ƒë·ªÅ/Khuy·∫øn ngh·ªã' ƒëa d·∫°ng theo KV & h·∫°ng m·ª•c b·ªã d∆∞·ªõi ng∆∞·ª°ng.
    """
    kv = _kv_for_variety(kv_key)
    th = AREA_RULE_THRESHOLDS.get(kv, _AREA_RULE_THRESHOLDS[kv])

    # Seed theo th·ªùi ƒëi·ªÉm ƒë·ªÉ c√¢u ch·ªØ ƒë·ªïi linh ho·∫°t
    random.seed(hash(f"{kv}{time.time_ns()}") % (2**32))

    issues, recs = [], []
    for metric, val in parts.items():
        thr = th.get(metric, 0.75)
        if float(val) < float(thr):  # d∆∞·ªõi ng∆∞·ª°ng ‚Üí n√™u v·∫•n ƒë·ªÅ & g·ª£i √Ω
            issues += _pick_many(SIMPLE_ISSUE_BANK.get(kv, {}).get(metric, []), k=2)
            recs   += _pick_many(SIMPLE_REC_BANK.get(kv, {}).get(metric, []),   k=2)

    # Kh·ª≠ tr√πng l·∫∑p & r√∫t g·ªçn t·ªëi ƒëa 5 √Ω m·ªói ph·∫ßn
    def _dedup(xs, limit=5):
        seen, out = set(), []
        for x in xs:
            if not x or x in seen: continue
            seen.add(x); out.append(x)
            if len(out) >= limit: break
        return out

    return _dedup(issues, 5), _dedup(recs, 5)
# ========== END DIAGNOSTICS VARIETY ==========
def apply_scoring_struct(photo_bytes: bytes, kv_active: str|None, is_duplicate: bool, dup_key: str, ngay_str: str):
    """
    Tr·∫£ v·ªÅ c·∫•u tr√∫c cho g·ªôp: {'total','grade','issues','recs','dup','sim','dup_date'}
    B·∫Øt bu·ªôc c√≥ v·∫•n ƒë·ªÅ/khuy·∫øn ngh·ªã n·∫øu total < 95.
    T√≠nh t∆∞∆°ng ƒë·ªìng ·∫£nh b·∫±ng pHash v√† l∆∞u l·ªãch s·ª≠ theo (chat_id|id_kho).
    """
    if not SCORING_ENABLED:
        return {'total': 0, 'grade': 'C', 'issues': [], 'recs': [], 'dup': is_duplicate, 'sim': 0.0, 'dup_date': None}

    # 1) ƒê·ªçc ·∫£nh + pHash
    img = cv2.imdecode(np.frombuffer(photo_bytes, np.uint8), cv2.IMREAD_COLOR)
    try:
        phash = _phash_cv(img)
    except Exception:
        phash = None

    # 2) Ch·∫•t l∆∞·ª£ng
    sharp_s, bright_s, size_s, (w, h) = _score_quality_components(img)
    q_score = 0.2 * (0.6 * sharp_s + 0.4 * bright_s)

    # 3) N·ªôi dung theo KV
    parts, kv_key = _score_by_kv(photo_bytes, kv_active or "")
    weights = AREA_RULE_WEIGHTS.get(kv_key, _DEFAULT_WEIGHTS[kv_key])
    total_w = float(sum(weights.values())) or 100.0
    content_s = 0.0
    for name, val in parts.items():
        w_part = float(weights.get(name, 0.0))
        content_s += (float(val) * (w_part / total_w) * 0.8)

    # 4) So tr√πng (pHash) tr√™n l·ªãch s·ª≠ nhi·ªÅu ng√†y
    sim_best, sim_date = 0.0, None
    if phash is not None and dup_key:
        sim_best, sim_date = _dup_best_match(dup_key, phash)
        if sim_best >= 0.90:
            is_duplicate = True

    # 5) T·ªïng ƒëi·ªÉm
    dup_penalty = 0.10 if is_duplicate else 0.0
    total_norm = max(0.0, q_score + content_s - dup_penalty)
    total = int(round(total_norm * 100))
    grade = "A" if total >= 80 else ("B" if total >= 65 else "C")

    # 6) V·∫•n ƒë·ªÅ / Khuy·∫øn ngh·ªã
    issues, recs = _diagnose_varied(kv_key, parts)
    if sharp_s < 0.80:
        issues.append("·∫¢nh h∆°i m·ªù/thi·∫øu n√©t")
        recs.append("Gi·ªØ ch·∫Øc tay ho·∫∑c t·ª±a v√†o b·ªÅ m·∫∑t; ch·ª•p g·∫ßn h∆°n n·∫øu c·∫ßn")
    if bright_s < 0.80:
        issues.append("·∫¢nh qu√° t·ªëi/ho·∫∑c qu√° s√°ng")
        recs.append("Ch·ª•p n∆°i ƒë·ªß s√°ng, tr√°nh ng∆∞·ª£c s√°ng; b·∫≠t ƒë√®n khu v·ª±c")
    if size_s < 1.0:
        issues.append("K√≠ch th∆∞·ªõc ·∫£nh nh·ªè/thi·∫øu chi ti·∫øt")
        recs.append("D√πng ƒë·ªô ph√¢n gi·∫£i cao h∆°n ho·∫∑c ƒë·ª©ng g·∫ßn ƒë·ªëi t∆∞·ª£ng h∆°n")
    if is_duplicate:
        pct = int(round(sim_best * 100)) if sim_best > 0 else None
        if pct is not None and sim_date:
            issues.append(f"·∫¢nh tr√πng ~{pct}% so v·ªõi ·∫£nh ƒë√£ g·ª≠i ng√†y {sim_date}")
            recs.append("Ch·ª•p l·∫°i ·∫£nh m·ªõi, ƒë·ªïi g√≥c ch·ª•p ƒë·ªÉ ph·∫£n √°nh hi·ªán tr·∫°ng")
        else:
            issues.append("·∫¢nh b·ªã tr√πng l·∫∑p v·ªõi ·∫£nh ƒë√£ g·ª≠i")
            recs.append("G·ª≠i ·∫£nh m·ªõi ch·ª•p cho khu v·ª±c t∆∞∆°ng ·ª©ng")

    if total < 95 and not issues:
        issues.append("ƒêi·ªÉm ch∆∞a ƒë·∫°t 95/100 theo chu·∫©n 5S")
        recs.append("Xem l·∫°i s·∫Øp x·∫øp/v·ªá sinh/l·ªëi ƒëi v√† ch·ª•p l·∫°i ·∫£nh r√µ h∆°n n·∫øu c·∫ßn")

    # 7) L∆∞u l·ªãch s·ª≠ pHash
    try:
        if phash is not None and dup_key:
            _dup_push(dup_key, phash, ngay_str)
    except Exception:
        pass

    return {'total': total, 'grade': grade, 'issues': issues, 'recs': recs, 'dup': is_duplicate, 'sim': sim_best, 'dup_date': sim_date}


def _compose_aggregate_message(items: list, id_kho: str, ngay_str: str) -> str:
    header = "üßÆ *ƒêi·ªÉm 5S cho l√¥ ·∫£nh n√†y*\n" + f"- Kho: `{id_kho}` ¬∑ Ng√†y: `{ngay_str}`\n"
    lines = []
    agg_issues, agg_recs = [], []
    for idx, it in enumerate(items, 1):
        if it.get('dup'):
            pct = int(round(it.get('sim',0)*100)) if it.get('sim') else None
            if pct and it.get('dup_date'):
                dup_txt = f"‚ùå ~{pct}% (·∫£nh ng√†y {it['dup_date']})"
            else:
                dup_txt = "‚ùå"
        else:
            dup_txt = "‚úÖ"
        lines.append(f"‚Ä¢ ·∫¢nh #{idx}: *{it['total']}/100* ‚Üí Lo·∫°i *{it['grade']}*")
        agg_issues.extend(it.get('issues', [])); agg_recs.extend(it.get('recs', []))
    def _uniq_first(xs, limit=5):
        seen, out = set(), []
        for x in xs:
            if not x or x in seen: continue
            seen.add(x); out.append(x)
            if len(out) >= limit: break
        return out
    issues_u = _uniq_first(agg_issues, 5); recs_u = _uniq_first(agg_recs, 5)
    msg = header + "\n" + "\n".join(lines)
    if issues_u or recs_u:
        msg += "\n\n‚ö†Ô∏è *V·∫•n ƒë·ªÅ:*" + "".join([f"\n ‚Ä¢ {x}" for x in issues_u])
        msg += "\n\nüõ†Ô∏è *Khuy·∫øn ngh·ªã:*" + "".join([f"\n ‚Ä¢ {x}" for x in recs_u])
    return msg

async def _send_scoring_aggregate(context: ContextTypes.DEFAULT_TYPE):
    data = context.job.data or {}
    key = data.get("key"); chat_id = data.get("chat_id"); id_kho = data.get("id_kho"); ngay_str = data.get("ngay")
    items = SCORING_BUFFER.pop(key, []); SCORING_JOBS.pop(key, None)
    if not items: return
    text = _compose_aggregate_message(items, id_kho, ngay_str)
    try:
        await context.bot.send_message(chat_id=chat_id, text=text, parse_mode=ParseMode.MARKDOWN)
    except Exception:
        pass

def schedule_scoring_aggregate(context, chat_id: int, id_kho: str, ngay_str: str, delay_seconds: int = 5):
    key = _scoring_key(chat_id, id_kho, ngay_str)
    old = SCORING_JOBS.get(key)
    if old:
        try: old.schedule_removal()
        except Exception: pass
    job = context.job_queue.run_once(
        _send_scoring_aggregate, when=delay_seconds,
        data={"key": key, "chat_id": chat_id, "id_kho": id_kho, "ngay": ngay_str},
        name=f"score_agg_{key}"
    )
    SCORING_JOBS[key] = job
# ==== H·∫æT PH·∫¶N G·ªòP ====
async def _warning_job(context):
    """Job ch·∫°y sau 6s k·ªÉ t·ª´ l·∫ßn ghi nh·∫≠n g·∫ßn nh·∫•t."""
    data = context.job.data or {}
    chat_id = data.get("chat_id")
    id_kho = data.get("id_kho")
    day = data.get("day")

    # ƒê·ªçc l·∫°i count m·ªõi nh·∫•t ƒë·ªÉ c·∫£nh b√°o ch√≠nh x√°c
    try:
        count_db = load_count_db()
        cur = int(count_db.get(day, {}).get(str(id_kho), 0))
    except Exception:
        cur = 0

    if cur > REQUIRED_PHOTOS:
        await context.bot.send_message(chat_id, f"‚ö†Ô∏è ƒê√£ g·ªüi qu√° s·ªë ·∫£nh so v·ªõi quy ƒë·ªãnh ( {REQUIRED_PHOTOS} ·∫£nh )")
    elif cur < REQUIRED_PHOTOS:
        await context.bot.send_message(chat_id, f"‚ö†Ô∏è C√≤n {REQUIRED_PHOTOS - cur} thi·∫øu 1 ·∫£nh so v·ªõi quy ƒë·ªãnh ( {REQUIRED_PHOTOS} ·∫£nh )")
    # = 4 th√¨ kh√¥ng g·ª≠i g√¨

def schedule_delayed_warning(context, chat_id, id_kho, d):
    """ƒê·∫∑t/c·∫≠p nh·∫≠t 1 job c·∫£nh b√°o ch·∫°y sau 6 gi√¢y."""
    key = (chat_id, str(id_kho), _day_key(d))
    # Hu·ª∑ job c≈© n·∫øu c√≥ ƒë·ªÉ ch·ªâ g·ª≠i 1 c·∫£nh b√°o sau l·∫ßn g·ª≠i cu·ªëi
    old = WARN_JOBS.pop(key, None)
    if old:
        try:
            old.schedule_removal()
        except Exception:
            pass
    # T·∫°o job m·ªõi (6s)
    job = context.job_queue.run_once(
        _warning_job, when=6,
        data={"chat_id": chat_id, "id_kho": str(id_kho), "day": _day_key(d)},
        name=f"warn_{chat_id}_{id_kho}_{_day_key(d)}"
    )
    WARN_JOBS[key] = job


# ========= JSON UTILS =========
def _load_json(path: str, default):
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return default

def _save_json(path: str, data):
    tmp = path + ".tmp"
    with open(tmp, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    os.replace(tmp, path)

def load_hash_db():
    return _load_json(HASH_DB_PATH, {"items": []})

def save_hash_db(db):
    _save_json(HASH_DB_PATH, db)

def load_submit_db():
    return _load_json(SUBMIT_DB_PATH, {})

def save_submit_db(db):
    _save_json(SUBMIT_DB_PATH, db)

def load_count_db():
    return _load_json(COUNT_DB_PATH, {})

def save_count_db(db):
    _save_json(COUNT_DB_PATH, db)

def load_past_db():
    return _load_json(PAST_DB_PATH, {})

def save_past_db(db):
    _save_json(PAST_DB_PATH, db)

# ========= KHO MAP =========
def load_kho_map():
    df = pd.read_excel(EXCEL_PATH)
    cols = {c.lower().strip(): c for c in df.columns}
    if "id_kho" not in cols or "ten_kho" not in cols:
        raise RuntimeError("Excel ph·∫£i c√≥ c·ªôt 'id_kho' v√† 'ten_kho'")
    id_col = cols["id_kho"]; ten_col = cols["ten_kho"]
    df = df[[id_col, ten_col]].dropna()
    df[id_col] = df[id_col].astype(str).str.strip()
    df[ten_col] = df[ten_col].astype(str).str.strip()
    return dict(zip(df[id_col], df[ten_col]))

# ========= PARSE TEXT =========
ID_RX = re.compile(r"(\d{1,10})")
DATE_RX = re.compile(r"(?:ng√†y|date|ngay)\s*[:\-]?\s*(\d{1,2})[\/\-](\d{1,2})[\/\-](\d{2,4})", re.IGNORECASE)

def parse_text_for_id_and_date(text: str):
    """Tr√≠ch ID kho & ng√†y (tu·ª≥ ch·ªçn 'Ng√†y: dd/mm/yyyy'). N·∫øu kh√¥ng c√≥ ng√†y -> h√¥m nay."""
    _id = None
    _date = date.today()
    if not text:
        return None, _date

    m_id = ID_RX.search(text)
    if m_id:
        _id = m_id.group(1)

    m_d = DATE_RX.search(text)
    if m_d:
        d, m, y = m_d.groups()
        d, m, y = int(d), int(m), int(y)
        if y < 100: y += 2000
        try:
            _date = date(y, m, d)
        except Exception:
            _date = date.today()

    return _id, _date

# ========= GI·ªÆ TEXT D√ôNG CHUNG =========
_last_text = {}  # chat_id -> (text, ts)

def upsert_last_text(chat_id: int, text: str):
    _last_text[chat_id] = (text, datetime.now(TZ).timestamp())

def get_last_text(chat_id: int):
    data = _last_text.get(chat_id)
    if not data:
        return None
    text, ts = data
    if datetime.now(TZ).timestamp() - ts > TEXT_PAIR_TIMEOUT:
        return None
    return text


# ========= 5S SCORING HELPERS (rule-based) =========
def _score_quality_components(img_bgr):
    if img_bgr is None or img_bgr.size == 0:
        return 0.0, 0.0, 0.0, (0,0)
    h, w = img_bgr.shape[:2]
    img_gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)
    sharp_val = float(cv2.Laplacian(img_gray, cv2.CV_64F).var())
    sharp_score = 1.0 if sharp_val >= LAPLACIAN_GOOD else max(0.0, sharp_val / LAPLACIAN_GOOD)
    mean_bright = float(img_gray.mean())
    if mean_bright < BRIGHT_MIN:
        bright_score = max(0.0, mean_bright/BRIGHT_MIN)
    elif mean_bright > BRIGHT_MAX:
        bright_score = max(0.0, (255.0-mean_bright)/(255.0-BRIGHT_MAX+1e-6))
    else:
        bright_score = 1.0
    short_edge = min(h, w)
    size_score = 1.0 if short_edge >= MIN_SHORT_EDGE else max(0.0, short_edge/MIN_SHORT_EDGE)
    return sharp_score, bright_score, size_score, (w,h)

def _edge_density(img_gray, t1=80, t2=200):
    edges = cv2.Canny(img_gray, t1, t2)
    return float((edges>0).mean())

def _score_hanghoa(img_bgr):
    gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)
    edges = cv2.Canny(gray, 80, 200)
    lines = cv2.HoughLines(edges, 1, np.pi/180, 120)
    angles = []
    if lines is not None:
        for l in lines[:100]:
            rho, theta = l[0]
            angles.append(theta)
    def angle_dev(theta):
        return min(abs(theta-0), abs(theta-np.pi/2))
    if angles:
        devs = np.array([angle_dev(t) for t in angles], dtype=np.float32)
        align_score = float(max(0.0, 1.0 - (devs.mean()/(np.pi/8))))
    else:
        align_score = 0.6
    clutter = _edge_density(gray)  # 0..1
    tidy_score = float(max(0.0, 1.0 - min(clutter/0.25, 1.0)))
    h, w = gray.shape
    lower = gray[int(h*0.55):, :]
    lower_edges = cv2.Canny(lower, 80, 200)
    empty_ratio = 1.0 - float((lower_edges>0).mean())
    aisle_score = float(max(0.0, min(empty_ratio, 1.0)))
    return {"align": align_score, "tidy": tidy_score, "aisle": aisle_score}

def _score_wc(img_bgr):
    gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)
    k = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (7,7))
    blackhat = cv2.morphologyEx(gray, cv2.MORPH_BLACKHAT, k)
    _, th = cv2.threshold(blackhat, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
    stain_ratio = float((th>0).mean())
    stain_score = float(max(0.0, 1.0 - min(stain_ratio/0.10, 1.0)))
    h, w = gray.shape
    lower = gray[int(h*0.55):, :]
    lower_blur = cv2.GaussianBlur(lower, (5,5), 0)
    _, lower_th = cv2.threshold(lower_blur, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)
    cnts, _ = cv2.findContours(lower_th, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    small_blobs = [c for c in cnts if 10 <= cv2.contourArea(c) <= 500]
    trash_density = float(len(small_blobs)) / max(1.0, (w*h/10000.0))
    trash_score = float(max(0.0, 1.0 - min(trash_density/1.5, 1.0)))
    lap = cv2.Laplacian(gray, cv2.CV_64F)
    local_var = float(np.var(lap))
    dry_score = float(max(0.0, 1.0 - min(local_var/500.0, 1.0)))
    return {"stain": stain_score, "trash": trash_score, "dry": dry_score}

def _score_khobai(img_bgr):
    gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)
    blur = cv2.GaussianBlur(gray, (5,5), 0)
    _, th = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
    dirt_ratio = float((th == 0).mean())
    clean_score = float(max(0.0, 1.0 - min(dirt_ratio/0.20, 1.0)))
    h, w = gray.shape
    band = gray[int(h*0.45):int(h*0.75), :]
    obs_density = _edge_density(band)
    obstacle_score = float(max(0.0, 1.0 - min(obs_density/0.30, 1.0)))
    edges = cv2.Canny(gray, 80, 200)
    lines = cv2.HoughLines(edges, 1, np.pi/180, 150)
    line_score = 0.6 if lines is None else 1.0
    return {"clean": clean_score, "obstacle": obstacle_score, "line": float(line_score)}

def _score_vanphong(img_bgr):
    gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)
    h, w = gray.shape
    band = gray[int(h*0.25):int(h*0.75), int(w*0.10):int(w*0.90)]
    band_blur = cv2.GaussianBlur(band, (5,5), 0)
    _, band_th = cv2.threshold(band_blur, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)
    cnts, _ = cv2.findContours(band_th, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    small_items = [c for c in cnts if 20 <= cv2.contourArea(c) <= 1500]
    item_density = float(len(small_items)) / max(1.0, (w*h/10000.0))
    desk_tidy = float(max(0.0, 1.0 - min(item_density/1.5, 1.0)))
    k = cv2.getStructuringElement(cv2.MORPH_RECT, (7,7))
    blackhat = cv2.morphologyEx(gray, cv2.MORPH_BLACKHAT, k)
    _, th = cv2.threshold(blackhat, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
    dirty_ratio = float((th>0).mean())
    surface_clean = float(max(0.0, 1.0 - min(dirty_ratio/0.15, 1.0)))
    margin = gray[:, :int(w*0.20)]
    edges = cv2.Canny(margin, 60, 160)
    cable_density = float((edges>0).mean())
    cable = float(max(0.0, 1.0 - min(cable_density/0.35, 1.0)))
    return {"desk_tidy": desk_tidy, "surface_clean": surface_clean, "cable": cable}

def _kv_key_from_text(kv_text):
    kv = (kv_text or "").strip().lower()
    if "hanghoa" in kv or "h√†ng" in kv or "hang" in kv:
        return "HangHoa"
    if "wc" in kv or "toilet" in kv or "vesinh" in kv or "v·ªá" in kv or "tolet" in kv:
        return "WC"
    if "kho" in kv or "kho b√£i" in kv:
        return "KhoBai"
    if "vƒÉn" in kv or "vanphong" in kv or "ban lam viec" in kv or "ban" in kv:
        return "VanPhong"
    return "HangHoa"  # default

def _score_by_kv(photo_bytes: bytes, kv_text: str):
    img = cv2.imdecode(np.frombuffer(photo_bytes, np.uint8), cv2.IMREAD_COLOR)
    kv_key = _kv_key_from_text(kv_text)
    if kv_key == "HangHoa":
        parts = _score_hanghoa(img)
    elif kv_key == "WC":
        parts = _score_wc(img)
    elif kv_key == "KhoBai":
        parts = _score_khobai(img)
    else:
        parts = _score_vanphong(img)
        kv_key = "VanPhong"
    return parts, kv_key

def _diagnose(kv_key: str, parts: dict):
    th = AREA_RULE_THRESHOLDS.get(kv_key, _AREA_RULE_THRESHOLDS[kv_key])
    issues, recs = [], []
    if kv_key == "HangHoa":
        if parts.get("align",1) < th["align"]:
            issues.append("H√†ng h√≥a ch∆∞a th·∫≥ng h√†ng / kh√¥ng song song k·ªá")
            recs.append("Ch·ªânh th·∫≥ng ki·ªán/th√πng theo line ho·∫∑c m√©p k·ªá; d√πng pallet m·ªôt h∆∞·ªõng")
        if parts.get("tidy",1) < th["tidy"]:
            issues.append("Khu v·ª±c b·ª´a b·ªôn, nhi·ªÅu v·∫≠t nh·ªè r·ªùi r·∫°c")
            recs.append("Gom th√πng r·ªóng, b·ªè v·∫≠t c·∫£n; ph√¢n khu r√µ theo lo·∫°i h√†ng")
        if parts.get("aisle",1) < th["aisle"]:
            issues.append("L·ªëi ƒëi b·ªã h·∫πp ho·∫∑c c√≥ v·∫≠t ch·∫Øn")
            recs.append("Gi·ªØ l·ªëi ƒëi th√¥ng tho√°ng (‚â• 1m), kh√¥ng x·∫øp h√†ng l·∫•n line")
    elif kv_key == "WC":
        if parts.get("stain",1) < th["stain"]:
            issues.append("B·ªìn/s√†n c√≥ v·∫øt b·∫©n/·ªë"); recs.append("C·ªç r·ª≠a b·ªìn, s√†n; d√πng dung d·ªãch t·∫©y r·ª≠a ƒë·ªãnh k·ª≥")
        if parts.get("trash",1) < th["trash"]:
            issues.append("C√≥ r√°c/gi·∫•y v·ª•n tr√™n s√†n"); recs.append("Thu gom r√°c; th√™m th√πng r√°c n·∫Øp; ƒë·ªï r√°c cu·ªëi ca")
        if parts.get("dry",1) < th["dry"]:
            issues.append("S√†n ∆∞·ªõt ho·∫∑c c√≤n v·ªát n∆∞·ªõc"); recs.append("Lau kh√¥ s√†n; treo bi·ªÉn c·∫£nh b√°o s√†n ∆∞·ªõt khi v·ªá sinh")
    elif kv_key == "KhoBai":
        if parts.get("clean",1) < th["clean"]:
            issues.append("S√†n kho b·∫©n / c√≥ nhi·ªÅu m·∫£ng t·ªëi"); recs.append("Qu√©t d·ªçn/lau s√†n theo t·∫ßn su·∫•t; x·ª≠ l√Ω d·∫ßu tr√†n ngay")
        if parts.get("obstacle",1) < th["obstacle"]:
            issues.append("C√≥ ch∆∞·ªõng ng·∫°i/l·ªôn x·ªôn ·ªü l·ªëi ƒëi"); recs.append("Di d·ªùi v·∫≠t c·∫£n; quy ƒë·ªãnh khu ƒë·∫∑t ƒë·ªì t·∫°m kh√¥ng l·∫•n line")
        if parts.get("line",1) < th["line"]:
            issues.append("Line k·∫ª ch·ªâ d·∫´n m·ªù/kh√≥ th·∫•y"); recs.append("S∆°n/k·∫ª l·∫°i line; b·ªï sung bi·ªÉn b√°o v·ªã tr√≠")
    else:  # VanPhong
        if parts.get("desk_tidy",1) < th["desk_tidy"]:
            issues.append("B√†n l√†m vi·ªác l·ªôn x·ªôn"); recs.append("S·∫Øp x·∫øp v·∫≠t d·ª•ng; d√πng khay/h·ªôp ph√¢n lo·∫°i; d·ªçn b√†n cu·ªëi ng√†y")
        if parts.get("surface_clean",1) < th["surface_clean"]:
            issues.append("B·ªÅ m·∫∑t c√≥ b·ª•i/v·∫øt b·∫©n"); recs.append("Lau b·ªÅ m·∫∑t b·∫±ng dung d·ªãch ph√π h·ª£p; l·ªãch v·ªá sinh h·∫±ng ng√†y")
        if parts.get("cable",1) < th["cable"]:
            issues.append("D√¢y ƒëi·ªán/c√°p l·ªôn x·ªôn"); recs.append("D√πng k·∫πp/·ªëng b·ªçc d√¢y; gom d√¢y v·ªÅ m·ªôt m√©p b√†n/ƒë·∫ø c·ªë ƒë·ªãnh")
    return issues, recs
def apply_scoring_rule(photo_bytes: bytes, kv_text: str, is_duplicate: bool=False):
    img = cv2.imdecode(np.frombuffer(photo_bytes, np.uint8), cv2.IMREAD_COLOR)
    sharp_s, bright_s, size_s, (w,h) = _score_quality_components(img)
    q_score = 0.2 * (0.6*sharp_s + 0.4*bright_s)
    kv_key = _kv_key_from_text(kv_text)
    if kv_key == "HangHoa":
        parts = _score_hanghoa(img)
    elif kv_key == "WC":
        parts = _score_wc(img)
    elif kv_key == "KhoBai":
        parts = _score_khobai(img)
    else:
        parts = _score_vanphong(img)
    weights = AREA_RULE_WEIGHTS.get(kv_key, _DEFAULT_WEIGHTS[kv_key])
    total_w = float(sum(weights.values())) or 100.0
    content_s = 0.0
    for name, val in parts.items():
        content_s += (float(val) * (float(weights.get(name,0))/total_w) * 0.8)
    # duplicate similarity across days
    sim_best, sim_date = (0.0, None)
    if 'phash' in locals() and phash is not None and dup_key:
        sim_best, sim_date = _dup_best_match(dup_key, phash)
        if sim_best >= 0.90:
            is_duplicate = True
    dup_penalty = 0.10 if is_duplicate else 0.0
    total_norm = max(0.0, q_score + content_s - dup_penalty)
    total = int(round(total_norm*100))
    grade = "A" if total >= 80 else ("B" if total >= 65 else "C")
    details = " ¬∑ ".join([f"{k}:{v:.2f}" for k,v in parts.items()])
    text = (
        "üßÆ *ƒêi·ªÉm 5S cho ·∫£nh n√†y*\n"
        f"- T·ªïng: *{total}/100* ‚Üí Lo·∫°i *{grade}*\n"
        f"- KV: `{kv_key}` ¬∑ H·∫°ng m·ª•c: {details}\n"
        f"- Ch·∫•t l∆∞·ª£ng: n√©t {sharp_s:.2f} ¬∑ s√°ng {bright_s:.2f} ¬∑ k√≠ch th∆∞·ªõc {w}√ó{h}"
    )
    return text
# ========= SUBMISSION/COUNTS =========
def mark_submitted(submit_db, id_kho: str, d: date):
    key = d.isoformat()
    lst = submit_db.get(key, [])
    if id_kho not in lst:
        lst.append(id_kho)
    submit_db[key] = lst

def inc_count(count_db, id_kho: str, d: date, step: int = 1) -> int:
    key = d.isoformat()
    day = count_db.get(key, {})
    cur = int(day.get(id_kho, 0)) + step
    day[id_kho] = cur
    count_db[key] = day
    return cur

def get_count(count_db, id_kho: str, d: date) -> int:
    return int(count_db.get(d.isoformat(), {}).get(id_kho, 0))

# ========= PAST-USE LOG =========
def log_past_use(id_kho: str, prev_date: str, h: str, today: date):
    db = load_past_db()
    key = today.isoformat()
    arr = db.get(key, [])
    arr.append({"id_kho": id_kho, "prev_date": prev_date, "hash": h})
    db[key] = arr
    save_past_db(db)

# ========= G·ªòP TIN NH·∫ÆN TI·∫æN ƒê·ªò (m·ªói kho/m·ªói ng√†y 1 tin) =========
PROGRESS_MSG = {}  # {(chat_id, id_kho, yyyy-mm-dd): {'msg_id': int|None, 'lines': list[str]}}

def day_key(d: date) -> str:
    return d.isoformat()  # YYYY-MM-DD

async def ack_photo_progress(context: ContextTypes.DEFAULT_TYPE, chat_id: int, id_kho: str, ten_kho: str, d: date, cur_count: int):
    """
    Gom to√†n b·ªô ti·∫øn ƒë·ªô g·ª≠i ·∫£nh c·ªßa 1 kho trong 1 ng√†y v√†o 1 tin nh·∫Øn.
    Kh√¥ng c√≤n c√¢u 'C√≤n thi·∫øu X ·∫£nh'.
    Khi ƒë·ªß REQUIRED_PHOTOS ·∫£nh th√¨ th√™m d√≤ng 'ƒê√É ƒê·ª¶ ... C·∫£m ∆°n b·∫°n!'.
    """
    key = (chat_id, str(id_kho), day_key(d))
    state = PROGRESS_MSG.setdefault(key, {'msg_id': None, 'lines': []})
    date_text = d.strftime("%d/%m/%Y")

    if cur_count < REQUIRED_PHOTOS:
        line = f"‚úÖ ƒê√£ ghi nh·∫≠n ·∫£nh {cur_count}/{REQUIRED_PHOTOS} cho {ten_kho} (ID `{id_kho}`) - Ng√†y {date_text}."
    else:
        line = f"‚úÖ ƒê√É ƒê·ª¶ {REQUIRED_PHOTOS}/{REQUIRED_PHOTOS} ·∫£nh cho {ten_kho} (ID `{id_kho}`) - Ng√†y {date_text}. C·∫£m ∆°n b·∫°n!"

    state['lines'].append(line)
    text = "\n".join(state['lines'])

    if state['msg_id'] is None:
        m = await context.bot.send_message(chat_id=chat_id, text=text, parse_mode=ParseMode.MARKDOWN, disable_web_page_preview=True)
        state['msg_id'] = m.message_id
    else:
        try:
            await context.bot.edit_message_text(chat_id=chat_id, message_id=state['msg_id'],
                                                text=text, parse_mode=ParseMode.MARKDOWN, disable_web_page_preview=True)
        except Exception:
            m = await context.bot.send_message(chat_id=chat_id, text=text, parse_mode=ParseMode.MARKDOWN, disable_web_page_preview=True)
            state['msg_id'] = m.message_id

# ========= HANDLERS =========
async def cmd_start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    msg = (
        "‚úÖ Bot s·∫µn s√†ng!\n\n"
        "*C√∫ ph√°p ƒë∆°n gi·∫£n (kh√¥ng c·∫ßn tag):*\n"
        "`<ID_KHO> - <T√™n kho>`\n"
        "`Ng√†y: dd/mm/yyyy` *(tu·ª≥ ch·ªçn)*\n\n"
        f"S·ªë ·∫£nh y√™u c·∫ßu m·ªói kho/ng√†y: *{REQUIRED_PHOTOS}*.\n"
        "‚û°Ô∏è M·∫πo: G·ª≠i 1 tin nh·∫Øn text c√≥ ID/Ng√†y r·ªìi g·ª≠i nhi·ªÅu ·∫£nh li√™n ti·∫øp (kh√¥ng caption) ‚Äî bot s·∫Ω √°p c√πng caption 2 ph√∫t.\n\n"
        "L·ªánh: `/chatid` l·∫•y chat id, `/report_now` g·ª≠i b√°o c√°o ngay."
    )
    await update.message.reply_text(msg, parse_mode=ParseMode.MARKDOWN)

async def cmd_help(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await cmd_start(update, context)

async def chatid(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await update.message.reply_text(str(update.effective_chat.id))

async def report_now(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await send_daily_report(context)
    await update.message.reply_text("‚úÖ ƒê√£ g·ª≠i b√°o c√°o 5S m·ªõi nh·∫•t v√†o c√°c group c·∫•u h√¨nh.")

async def text_handler(update: Update, context: ContextTypes.DEFAULT_TYPE):
    text = (update.message.text or "").strip()
    if text:
        upsert_last_text(update.effective_chat.id, text)

    id_kho, d = parse_text_for_id_and_date(text)
    kho_map = context.bot_data["kho_map"]

    if not id_kho:
        return  # kh√¥ng l√†m phi·ªÅn

    if id_kho not in kho_map:
        await update.message.reply_text(
            f"‚ùå ID `{id_kho}` *kh√¥ng c√≥* trong danh s√°ch. Ki·ªÉm tra l·∫°i!",
            parse_mode=ParseMode.MARKDOWN
        )
        return

    cur = get_count(load_count_db(), id_kho, d)
    await update.message.reply_text(
        f"‚úÖ ƒê√£ nh·∫≠n ID `{id_kho}` ({kho_map[id_kho]}). H√¥m nay hi·ªán c√≥ *{cur} / {REQUIRED_PHOTOS}* ·∫£nh. "
        "G·ª≠i ·∫£nh ngay sau ƒë√≥ (kh√¥ng c·∫ßn caption).",
        parse_mode=ParseMode.MARKDOWN
    )

async def photo_handler(update: Update, context: ContextTypes.DEFAULT_TYPE):
    msg = update.message

    # ---- ALBUM / MEDIA GROUP ----
    caption = (msg.caption or "").strip()
    caption_from_group = caption
    mgid = msg.media_group_id
    if mgid:
        albums = context.chat_data.setdefault("albums", {})
        rec = albums.setdefault(mgid, {"caption": None, "ts": datetime.now(TZ).timestamp()})
        if caption and not rec["caption"]:
            rec["caption"] = caption
        if not caption and rec["caption"]:
            caption_from_group = rec["caption"]

    # ---- FALLBACK: d√πng text ƒë√£ l∆∞u trong 2 ph√∫t ----
    if not caption_from_group:
        caption_from_group = get_last_text(msg.chat_id) or ""

    # parse
    id_kho, d = parse_text_for_id_and_date(caption_from_group)
    kho_map = context.bot_data["kho_map"]

    if not id_kho:
        await msg.reply_text(
            "‚ö†Ô∏è *Thi·∫øu ID kho.* Th√™m ID v√†o caption ho·∫∑c g·ª≠i 1 text c√≥ ID tr∆∞·ªõc r·ªìi g·ª≠i ·∫£nh (trong 2 ph√∫t).",
            parse_mode=ParseMode.MARKDOWN
        )
        return

    if id_kho not in kho_map:
        await msg.reply_text(
            f"‚ùå ID `{id_kho}` *kh√¥ng c√≥* trong danh s√°ch Excel. Ki·ªÉm tra l·∫°i!",
            parse_mode=ParseMode.MARKDOWN
        )
        return

    # t·∫£i bytes ·∫£nh & hash
    photo = msg.photo[-1]
    tg_file = await context.bot.get_file(photo.file_id)
    b = await tg_file.download_as_bytearray()
    b = bytes(b)
    h = hashlib.md5(b).hexdigest()

    # ===== C·∫¢NH B√ÅO TR√ôNG TRONG C√ôNG L√î (album) =====
    mg_hashes = context.chat_data.setdefault("mg_hashes", {})
    if mgid:
        seen = mg_hashes.setdefault(mgid, set())
        if h in seen:
            await msg.reply_text(
                "‚ö†Ô∏è C√≥ √≠t nh·∫•t 2 ·∫£nh *gi·ªëng nhau* trong c√πng l√¥ g·ª≠i. Vui l√≤ng ch·ªçn ·∫£nh kh√°c.",
                parse_mode=ParseMode.MARKDOWN
            )
            return
        seen.add(h)

    hash_db = load_hash_db()

    # ===== TR√ôNG TRONG NG√ÄY / L·ªäCH S·ª¨ =====
    # Tr√πng c√πng ng√†y/kho
    same_day_dups = [
        item for item in hash_db["items"]
        if item.get("hash") == h and item.get("id_kho") == id_kho and item.get("date") == d.isoformat()
    ]
    if same_day_dups:
        await msg.reply_text(
            f"‚ö†Ô∏è *{kho_map[id_kho]}* h√¥m nay ƒë√£ c√≥ 1 ·∫£nh *gi·ªëng h·ªát* ·∫£nh n√†y. Vui l√≤ng thay ·∫£nh kh√°c.",
            parse_mode=ParseMode.MARKDOWN
        )
        return

    # Tr√πng l·ªãch s·ª≠ -> log qu√° kh·ª© (l·∫•y ng√†y s·ªõm nh·∫•t)
    dups = [item for item in hash_db["items"] if item.get("hash") == h]
    if dups:
        prev_dates = sorted(set([it.get("date") for it in dups if it.get("date") != d.isoformat()]))
        if prev_dates:
            log_past_use(id_kho=id_kho, prev_date=prev_dates[0], h=h, today=d)
        await msg.reply_text(
            "‚ö†Ô∏è ·∫¢nh *tr√πng* v·ªõi ·∫£nh ƒë√£ g·ª≠i tr∆∞·ªõc ƒë√¢y. Vui l√≤ng ch·ª•p ·∫£nh m·ªõi kh√°c ƒë·ªÉ tr√°nh tr√πng l·∫∑p.",
            parse_mode=ParseMode.MARKDOWN
        )
        return

    # ===== GHI NH·∫¨N ·∫¢NH H·ª¢P L·ªÜ =====
    # ghi nh·∫≠n n·ªôp
    submit_db = load_submit_db()
    mark_submitted(submit_db, id_kho, d)
    save_submit_db(submit_db)

    # l∆∞u hash
    info = {
        "ts": datetime.now(TZ).isoformat(timespec="seconds"),
        "chat_id": msg.chat_id,
        "user_id": msg.from_user.id,
        "id_kho": id_kho,
        "date": d.isoformat(),
    }
    hash_db["items"].append({"hash": h, **info})
    save_hash_db(hash_db)

    # ƒë·∫øm s·ªë ·∫£nh v√† **G·ªòP** ph·∫£n h·ªìi theo kho/ng√†y
    count_db = load_count_db()
    cur = inc_count(count_db, id_kho, d, step=1)
    save_count_db(count_db)

    
    # ===== CH·∫§M ƒêI·ªÇM 5S (rule-based, kh√¥ng ML) =====
    if SCORING_ENABLED and SCORING_MODE == "rule":
        # L·∫•y KV t·ª´ caption/text n·∫øu c√≥
        kv_text = None
        m_kv = AREA_RX.search(caption_from_group or "")
        if m_kv:
            kv_text = m_kv.group(1)
        reply_text = apply_scoring_rule(b, kv_text or "", is_duplicate=False)
        dupkey = _dup_key(msg.chat_id, str(id_kho))
        ngay_text = d.strftime('%d/%m/%Y')
        item = apply_scoring_struct(b, kv_text or "", False, dupkey, ngay_text)
        key = _scoring_key(msg.chat_id, str(id_kho), ngay_text)
        SCORING_BUFFER[key].append(item)

    await ack_photo_progress(context, msg.chat_id, id_kho, kho_map[id_kho], d, cur)
    # ƒê·∫∑t c·∫£nh b√°o tr·ªÖ 6s sau m·ªói l·∫ßn ghi nh·∫≠n (job s·∫Ω t·ª± ki·ªÉm tra v√† ch·ªâ g·ª≠i n·∫øu <4 ho·∫∑c >4)
    schedule_delayed_warning(context, msg.chat_id, id_kho, d)

    # G·ª≠i ƒë√°nh gi√° 5S th√†nh 1 tin nh·∫Øn, tr·ªÖ 5 gi√¢y sau khi b√°o ghi nh·∫≠n
    if SCORING_ENABLED and SCORING_MODE == "rule":
        try:
            schedule_scoring_aggregate(context, chat_id=msg.chat_id, id_kho=str(id_kho), ngay_str=d.strftime('%d/%m/%Y'), delay_seconds=5)
        except Exception:
            pass
# ========= B√ÅO C√ÅO 21:00 =========
def get_missing_ids_for_day(kho_map, submit_db, d: date):
    submitted = set(submit_db.get(d.isoformat(), []))
    all_ids = set(kho_map.keys())
    return sorted(all_ids - submitted)

async def send_daily_report(context: ContextTypes.DEFAULT_TYPE):
    # danh s√°ch chat nh·∫≠n b√°o c√°o
    chat_ids = DEFAULT_REPORT_CHAT_IDS[:]
    env = os.getenv("REPORT_CHAT_IDS", "").strip()
    if env:
        chat_ids = [int(x.strip()) for x in env.split(",") if x.strip()]
    if not chat_ids:
        return

    kho_map = context.bot_data["kho_map"]
    submit_db = load_submit_db()
    count_db = load_count_db()
    past_db = load_past_db()
    today = datetime.now(TZ).date()

    # 1) Ch∆∞a b√°o c√°o
    missing_ids = get_missing_ids_for_day(kho_map, submit_db, today)

    # 2) ·∫¢nh c≈©/qu√° kh·ª©: gom theo kho, l·∫•y 1 ng√†y ƒë·∫°i di·ªán (s·ªõm nh·∫•t) ƒë·ªÉ b√°o g·ªçn
    past_uses = past_db.get(today.isoformat(), [])
    past_by_kho = {}
    for it in past_uses:
        kid = it.get("id_kho"); prev = it.get("prev_date")
        if not kid or not prev:
            continue
        s = past_by_kho.setdefault(kid, set())
        s.add(prev)
    past_lines = []
    for kid, dates in sorted(past_by_kho.items()):
        rep = min(dates)
        rep_str = datetime.fromisoformat(rep).strftime("%d/%m/%Y")
        past_lines.append(f"- `{kid}`: tr√πng ·∫£nh ng√†y {rep_str}")

    # 3) CH·ªà li·ªát k√™ CH∆ØA ƒê·ª¶ s·ªë ·∫£nh
    not_enough_list = []
    day_counts = count_db.get(today.isoformat(), {})
    for kid in kho_map.keys():
        c = int(day_counts.get(kid, 0))
        if 0 < c < REQUIRED_PHOTOS:
            not_enough_list.append((kid, c))

    parts = []
    # 1) Ch∆∞a b√°o c√°o 5S ‚Äî HI·ªÇN TH·ªä ID - T√äN KHO
    if missing_ids:
        lines = ["*1) C√°c kho ch∆∞a b√°o c√°o 5S:*"]
        for mid in missing_ids:
            name = kho_map.get(mid, "(kh√¥ng r√µ)")
            lines.append(f"- `{mid}` - {name}")
        parts.append("\n".join(lines))
    else:
        parts.append("*1) C√°c kho ch∆∞a b√°o c√°o 5S:* Kh√¥ng c√≥")

    # 2) ·∫¢nh c≈©/qu√° kh·ª©
    if past_lines:
        parts.append("*2) Kho s·ª≠ d·ª•ng ·∫£nh c≈©/qu√° kh·ª©:*\n" + "\n".join(past_lines))
    else:
        parts.append("*2) Kho s·ª≠ d·ª•ng ·∫£nh c≈©/qu√° kh·ª©:* Kh√¥ng c√≥")

    # 3) Ch∆∞a ƒë·ªß
    if not_enough_list:
        sec3 = ["*3) C√°c kho ch∆∞a g·ª≠i ƒë·ªß s·ªë l∆∞·ª£ng ·∫£nh:*"]
        for kid, c in sorted(not_enough_list):
            sec3.append(f"- `{kid}`: {c}/{REQUIRED_PHOTOS}")
        parts.append("\n".join(sec3))
    else:
        parts.append("*3) T·∫•t c·∫£ kho ƒë√£ g·ª≠i ƒë·ªß s·ªë l∆∞·ª£ng ·∫£nh theo quy ƒë·ªãnh*")

    text = f"üì¢ *B√ÅO C√ÅO 5S - {today.strftime('%d/%m/%Y')}*\n\n" + "\n\n".join(parts)

    for cid in chat_ids:
        try:
            await context.bot.send_message(cid, text, parse_mode=ParseMode.MARKDOWN)
        except Exception:
            pass

# ========= MAIN =========
def build_app() -> Application:
    token = os.getenv("BOT_TOKEN", "").strip()
    if not token:
        raise RuntimeError("Thi·∫øu bi·∫øn m√¥i tr∆∞·ªùng BOT_TOKEN")

    app = ApplicationBuilder().token(token).build()
    app.bot_data["kho_map"] = load_kho_map()

    app.add_handler(CommandHandler("start", cmd_start))
    app.add_handler(CommandHandler("help", cmd_help))
    app.add_handler(CommandHandler("chatid", chatid))
    app.add_handler(CommandHandler("report_now", report_now))
    app.add_handler(MessageHandler(filters.PHOTO & ~filters.COMMAND, photo_handler))
    app.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, text_handler))

    app.job_queue.run_daily(
        send_daily_report,
        time=dtime(hour=REPORT_HOUR, minute=0, tzinfo=TZ),
        name="daily_report_21h"
    )
    return app

def main():
    app = build_app()
    print("Bot is running...")
    app.run_polling(close_loop=False)

if __name__ == "__main__":
    main()


# ======= SIMPLE BANKS (ng·∫Øn g·ªçn, d·ªÖ hi·ªÉu, 10‚Äì20 √Ω m·ªói nh√≥m) =======
SIMPLE_SIMPLE_ISSUE_BANK = {
    "VanPhong": {
        "tidy": [
            "B√†n c√≥ nhi·ªÅu b·ª•i","Gi·∫•y t·ªù ƒë·ªÉ l·ªôn x·ªôn","D·ª•ng c·ª• ch∆∞a g·ªçn","M√†n h√¨nh ch∆∞a s·∫°ch","D√¢y c√°p r·ªëi",
            "Ly t√°ch, th·ª©c ƒÉn ƒë·ªÉ tr√™n b√†n","KhƒÉn gi·∫•y b·ª´a b·ªôn","NgƒÉn k√©o l·ªôn x·ªôn","B·ªÅ m·∫∑t d√≠nh b·∫©n","B√†n ph√≠m/b√†n di b·∫©n",
            "Gh·∫ø kh√¥ng ngay v·ªã tr√≠","Th√πng r√°c ƒë·∫ßy","Nhi·ªÅu v·∫≠t nh·ªè r∆°i v√£i","K·ªá t√†i li·ªáu ch∆∞a ph√¢n khu","B·∫£ng ghi ch√∫ r·ªëi m·∫Øt"
        ],
        "align": [
            "V·∫≠t d·ª•ng ƒë·∫∑t ch∆∞a ngay ng·∫Øn","ƒê·ªì ƒë·∫°c l·ªách v·ªã tr√≠","T√†i li·ªáu ch∆∞a x·∫øp th·∫≥ng m√©p",
            "M√†n h√¨nh/ƒë·∫ø ƒë·ª° l·ªách","B√∫t, s·ªï kh√¥ng theo h√†ng"
        ],
        "aisle": [
            "L·ªëi ƒëi b·ªã v∆∞·ªõng ƒë·ªì","C√≥ v·∫≠t c·∫£n d∆∞·ªõi ch√¢n b√†n","D√¢y ƒëi·ªán v·∫Øt ngang l·ªëi ƒëi",
            "Th√πng carton ch·∫Øn l·ªëi","T√∫i ƒë·ªì ƒë·ªÉ d∆∞·ªõi ch√¢n gh·∫ø"
        ]
    },
    "WC": {
        "stain": [
            "B·ªìn/b·ªÅ m·∫∑t c√≤n v·∫øt b·∫©n","G∆∞∆°ng, tay n·∫Øm ch∆∞a s·∫°ch","V·∫øt ·ªë quanh v√≤i","V·ªát n∆∞·ªõc tr√™n g∆∞∆°ng",
            "V√°ch ngƒÉn b√°m b·∫©n","S√†n b√°m c·∫∑n"
        ],
        "trash": [
            "Th√πng r√°c ƒë·∫ßy","R√°c ch∆∞a gom","T√∫i r√°c kh√¥ng thay","R√°c r∆°i ra ngo√†i"
        ],
        "dry": [
            "S√†n c√≤n ∆∞·ªõt","C√≥ v·ªát n∆∞·ªõc ƒë·ªçng","KhƒÉn gi·∫•y r∆°i xu·ªëng s√†n","Ch∆∞a ƒë·∫∑t bi·ªÉn c·∫£nh b√°o khi s√†n ∆∞·ªõt"
        ],
        "supply": [
            "Thi·∫øu gi·∫•y/ x√† ph√≤ng","Kh√¥ng c√≥ khƒÉn lau tay","B√¨nh x·ªãt tr·ªëng","Ch∆∞a b·ªï sung v·∫≠t t∆∞"
        ]
    },
    "HangHoa": {
        "align": [
            "H√†ng ch∆∞a th·∫≥ng h√†ng","Pallet xoay kh√°c h∆∞·ªõng","C√≥ kho·∫£ng h·ªü trong d√£y x·∫øp","Th√πng nh√¥ ra m√©p k·ªá",
            "Ki·ªán cao th·∫•p kh√¥ng ƒë·ªÅu","H√†ng l·ªách line v·∫°ch","Th√πng x·∫πp/bi·∫øn d·∫°ng","X·∫øp ch·ªìng m·∫•t c√¢n b·∫±ng"
        ],
        "tidy": [
            "Khu v·ª±c c√≤n b·ª´a b·ªôn","Th√πng r·ªóng ch∆∞a gom","V·∫≠t t·∫°m ƒë·∫∑t sai ch·ªó","M√†ng PE r√°ch v∆∞∆°ng v√£i",
            "D·ª•ng c·ª• ch∆∞a tr·∫£ v·ªÅ v·ªã tr√≠","Bao b√¨ r√°ch nh∆∞ng ch∆∞a x·ª≠ l√Ω","Nh√£n m√°c bong tr√≥c","C√≥ h√†ng ƒë·∫∑t tr·ª±c ti·∫øp xu·ªëng s√†n"
        ],
        "aisle": [
            "L·ªëi ƒëi b·ªã l·∫•n","ƒê·ªì c·∫£n tr·ªü ƒë∆∞·ªùng ƒëi","Pallet ƒë·ªÉ d∆∞·ªõi line","H√†ng ƒë·∫©y qua v·∫°ch an to√†n",
            "Khu v·ª±c thao t√°c ch·∫≠t h·∫πp"
        ],
        "bulky": [
            "H√†ng c·ªìng k·ªÅnh ch∆∞a c·ªë ƒë·ªãnh","D√¢y ƒëai l·ªèng","ƒêi·ªÉm t·ª±a kh√¥ng ch·∫Øc","ƒê·∫∑t sai h∆∞·ªõng n√¢ng h·∫°",
            "Thi·∫øu n·∫πp g√≥c/ƒë·ªám b·∫£o v·ªá","Ch∆∞a d√°n nh√£n c·∫£nh b√°o k√≠ch th∆∞·ªõc/t·∫£i tr·ªçng"
        ]
    },
    "LoiDi": {
        "aisle": [
            "L·ªëi ƒëi c√≥ v·∫≠t c·∫£n","V·∫°ch s∆°n m·ªù","H√†ng l·∫•n sang l·ªëi ƒëi","C√≥ ch·∫•t l·ªèng r∆°i v√£i",
            "Thi·∫øu bi·ªÉn ch·ªâ d·∫´n","L·ªëi tho√°t hi·ªÉm ch∆∞a th√¥ng tho√°ng","Xe ƒë·∫©y d·ª´ng sai v·ªã tr√≠"
        ]
    },
    "KePallet": {
        "align": [
            "Pallet kh√¥ng ngay h√†ng","C·∫°nh pallet l·ªách m√©p k·ªá","Ki·ªán ch·ªìng qu√° cao","Thanh gi·∫±ng kh√¥ng c√¢n ƒë·ªëi"
        ],
        "tidy": [
            "Pallet h·ªèng ch∆∞a lo·∫°i b·ªè","M·∫£nh g·ªó v·ª•n tr√™n s√†n","Tem c≈© ch∆∞a b√≥c","M√†ng PE d∆∞ ch∆∞a x·ª≠ l√Ω"
        ]
    }
}

SIMPLE_SIMPLE_REC_BANK = {
    "VanPhong": {
        "tidy": [
            "Lau b·ª•i b·ªÅ m·∫∑t","X·∫øp gi·∫•y t·ªù theo nh√≥m","C·∫•t d·ª•ng c·ª• v√†o khay","Lau s·∫°ch m√†n h√¨nh","Bu·ªôc g·ªçn d√¢y c√°p",
            "B·ªè th·ª©c ƒÉn/ly t√°ch ƒë√∫ng ch·ªó","D√°n nh√£n khay/ngƒÉn k√©o","D·ªçn r√°c ngay","D√πng khƒÉn lau kh·ª≠ khu·∫©n",
            "S·∫Øp x·∫øp b√∫t, s·ªï v√†o ·ªëng/k·ªá"
        ],
        "align": [
            "ƒê·∫∑t ƒë·ªì ngay ng·∫Øn","C·ªë ƒë·ªãnh v·ªã tr√≠ d√πng th∆∞·ªùng xuy√™n","CƒÉn th·∫≥ng theo m√©p b√†n/k·ªá",
            "D√πng khay chia √¥ cho ph·ª• ki·ªán"
        ],
        "aisle": [
            "D·∫πp v·∫≠t c·∫£n kh·ªèi l·ªëi ƒëi","B√≥ g·ªçn d√¢y ƒëi·ªán s√°t t∆∞·ªùng","Kh√¥ng ƒë·∫∑t th√πng/h·ªôp d∆∞·ªõi l·ªëi ch√¢n",
            "T·∫≠n d·ª•ng k·ªá treo cho ƒë·ªì l·∫∑t v·∫∑t"
        ]
    },
    "WC": {
        "stain": [
            "C·ªç r·ª≠a b·∫±ng dung d·ªãch ph√π h·ª£p","Lau g∆∞∆°ng, tay n·∫Øm","Ch√† s·∫°ch v·∫øt ·ªë quanh v√≤i",
            "V·ªá sinh v√°ch ngƒÉn v√† s√†n"
        ],
        "trash": [
            "ƒê·ªï r√°c ngay","Thay t√∫i r√°c m·ªõi","ƒê·∫∑t th√πng c√≥ n·∫Øp"
        ],
        "dry": [
            "Lau kh√¥ s√†n","ƒê·∫∑t bi·ªÉn c·∫£nh b√°o khi s√†n ∆∞·ªõt","Ki·ªÉm tra r√≤ r·ªâ, x·ª≠ l√Ω ngay"
        ],
        "supply": [
            "B·ªï sung gi·∫•y/ x√† ph√≤ng","Th√™m khƒÉn lau tay","N·∫°p ƒë·∫ßy b√¨nh x·ªãt"
        ]
    },
    "HangHoa": {
        "align": [
            "CƒÉn theo m√©p k·ªá/v·∫°ch","Xoay c√πng m·ªôt h∆∞·ªõng","B·ªï sung n·∫πp g√≥c gi·ªØ th·∫≥ng","San ph·∫≥ng chi·ªÅu cao ch√™nh l·ªách"
        ],
        "tidy": [
            "Gom th√πng r·ªóng v·ªÅ khu t·∫≠p k·∫øt","D·ªçn v·∫≠t t·∫°m ƒë·∫∑t sai ch·ªó","Qu·∫•n l·∫°i m√†ng PE g·ªçn g√†ng",
            "In/d√°n l·∫°i nh√£n m√°c r√µ r√†ng","ƒê·∫∑t h√†ng tr√™n pallet, kh√¥ng ƒë·∫∑t s√†n"
        ],
        "aisle": [
            "Gi·ªØ l·ªëi ƒëi th√¥ng tho√°ng","Di d·ªùi v·∫≠t c·∫£n kh·ªèi line","Ch·ª´a kho·∫£ng an to√†n ‚â• 1m"
        ],
        "bulky": [
            "ƒêai c·ªë ƒë·ªãnh ch·∫Øc ch·∫Øn","Th√™m n·∫πp g√≥c/ƒë·ªám b·∫£o v·ªá","ƒê·∫∑t h∆∞·ªõng thu·∫≠n l·ª£i n√¢ng h·∫°",
            "Ghi ch√∫ k√≠ch th∆∞·ªõc/t·∫£i tr·ªçng r√µ r√†ng","B·ªï sung ƒëi·ªÉm ch√®n ch·ªëng x√™ d·ªãch"
        ]
    },
    "LoiDi": {
        "aisle": [
            "D·ªçn s·∫°ch v·∫≠t c·∫£n","S∆°n l·∫°i v·∫°ch d·∫´n h∆∞·ªõng","ƒê·∫∑t l·∫°i h√†ng v∆∞·ª£t v·∫°ch","Lau s·∫°ch ch·∫•t l·ªèng r∆°i v√£i",
            "ƒê·∫£m b·∫£o l·ªëi tho√°t hi·ªÉm th√¥ng su·ªët","Quy ƒë·ªãnh v·ªã tr√≠ d·ª´ng cho xe ƒë·∫©y"
        ]
    },
    "KePallet": {
        "align": [
            "CƒÉn th·∫≥ng m√©p pallet","Kh√¥ng ch·ªìng qu√° quy ƒë·ªãnh","Ki·ªÉm tra thanh gi·∫±ng, c√¢n ch·ªânh"
        ],
        "tidy": [
            "Lo·∫°i b·ªè pallet h·ªèng","Qu√©t d·ªçn m·∫£nh g·ªó v·ª•n","C·∫Øt b·ªè m√†ng PE th·ª´a","B√≥c tem c≈© tr∆∞·ªõc khi d√°n tem m·ªõi"
        ]
    }
}
# ======= END SIMPLE BANKS =======



# ========= USER SIMPLE PHRASES (ng·∫Øn g·ªçn ‚Äì ƒëa d·∫°ng, ∆∞u ti√™n HangHoa) =========
def _prepend_unique(dst: dict, kv: str, cat: str, items: list):
    kvd = dst.setdefault(kv, {})
    arr = kvd.setdefault(cat, [])
    for s in reversed(items):
        if s not in arr:
            arr.insert(0, s)

def _apply_user_simple_overlay_all():
    # ===== HANG HOA (∆∞u ti√™n h√†ng c·ªìng k·ªÅnh) =====
    _prepend_unique(SIMPLE_ISSUE_BANK, "HangHoa", "tidy", [
        "H√†ng h√≥a kh√¥ng ƒë∆∞·ª£c s·∫Øp x·∫øp g·ªçn g√†ng",
        "C·∫ßn c·∫£i thi·ªán v·ªá sinh khu v·ª±c",
        "Thi·∫øu nh√£n m√°c cho h√†ng h√≥a",
        "Th√πng r·ªóng ch∆∞a gom",
        "M√†ng PE th·ª´a/ch∆∞a c·∫Øt g·ªçn",
        "Bao b√¨ r√°ch ch∆∞a x·ª≠ l√Ω",
        "D·ª•ng c·ª• t·∫°m ƒë·∫∑t sai v·ªã tr√≠",
        "Khu v·ª±c ch·∫•t h√†ng b·ª´a b·ªôn",
        "C√≥ h√†ng ƒë·∫∑t tr·ª±c ti·∫øp xu·ªëng s√†n",
        "Tem c≈© ch∆∞a b√≥c tr∆∞·ªõc khi d√°n tem m·ªõi"
    ])
    _prepend_unique(SIMPLE_ISSUE_BANK, "HangHoa", "align", [
        "M·ªôt s·ªë pallet c√≥ h√†ng h√≥a ch·∫•t ƒë·ªëng",
        "H√†ng kh√¥ng th·∫≥ng h√†ng theo m√©p k·ªá",
        "Pallet xoay kh√°c h∆∞·ªõng c√≤n l·∫°i",
        "C√≥ kho·∫£ng h·ªü gi·ªØa c√°c ki·ªán",
        "Ki·ªán ch·ªìng cao, d·ªÖ m·∫•t c√¢n b·∫±ng",
        "Th√πng nh√¥ ra m√©p pallet",
        "X·∫øp ch·ªìng ch∆∞a ƒë·ªìng ƒë·ªÅu chi·ªÅu cao",
        "N·∫πp g√≥c thi·∫øu ho·∫∑c l·ªèng",
        "Th√πng m√©o/x·∫πp ·∫£nh h∆∞·ªüng x·∫øp ch·ªìng",
        "H√†ng ƒë·∫∑t l·ªách line ƒë√°nh d·∫•u"
    ])
    _prepend_unique(SIMPLE_ISSUE_BANK, "HangHoa", "aisle", [
        "Kh√¥ng c√≥ l·ªëi ƒëi r√µ r√†ng gi·ªØa c√°c khu v·ª±c",
        "L·ªëi ƒëi b·ªã l·∫•n b·ªüi h√†ng h√≥a",
        "V·∫°ch an to√†n m·ªù/kh√≥ th·∫•y",
        "C√≥ v·∫≠t c·∫£n trong ƒë∆∞·ªùng ƒëi xe n√¢ng",
        "Ch·∫•t l·ªèng r∆°i v√£i tr√™n s√†n",
        "H√†ng v∆∞·ª£t qua v·∫°ch gi·ªõi h·∫°n"
    ])
    _prepend_unique(SIMPLE_ISSUE_BANK, "HangHoa", "bulky", [
        "H√†ng c·ªìng k·ªÅnh ch∆∞a c·ªë ƒë·ªãnh",
        "D√¢y ƒëai l·ªèng ho·∫∑c thi·∫øu",
        "Thi·∫øu n·∫πp g√≥c cho ki·ªán l·ªõn",
        "ƒê·∫∑t sai h∆∞·ªõng n√¢ng h·∫°",
        "Thi·∫øu c·∫£nh b√°o k√≠ch th∆∞·ªõc/t·∫£i tr·ªçng",
        "ƒêi·ªÉm t·ª±a/ƒë·ªám k√™ kh√¥ng ch·∫Øc ch·∫Øn"
    ])

    _prepend_unique(SIMPLE_REC_BANK, "HangHoa", "tidy", [
        "S·∫Øp x·∫øp h√†ng h√≥a theo lo·∫°i v√† k√≠ch th∆∞·ªõc",
        "D·ªçn d·∫πp khu v·ª±c ƒë·ªÉ ƒë·∫£m b·∫£o s·∫°ch s·∫Ω",
        "Th√™m nh√£n m√°c cho h√†ng h√≥a",
        "Th·ª±c hi·ªán ki·ªÉm tra ƒë·ªãnh k·ª≥ v·ªÅ 5S",
        "Gom th√πng r·ªóng v·ªÅ khu t·∫≠p k·∫øt",
        "C·∫Øt g·ªçn m√†ng PE th·ª´a",
        "D√°n l·∫°i nh√£n r√µ r√†ng, d·ªÖ ƒë·ªçc",
        "Lo·∫°i b·ªè bao b√¨ r√°ch, thay m·ªõi",
        "Thu h·ªìi d·ª•ng c·ª• v·ªÅ ƒë√∫ng v·ªã tr√≠",
        "Kh√¥ng ƒë·∫∑t h√†ng tr·ª±c ti·∫øp xu·ªëng s√†n"
    ])
    _prepend_unique(SIMPLE_REC_BANK, "HangHoa", "align", [
        "CƒÉn th·∫≥ng theo m√©p k·ªá/v·∫°ch ch·ªâ d·∫´n",
        "Xoay c√πng m·ªôt h∆∞·ªõng cho to√†n b·ªô ki·ªán",
        "San ph·∫≥ng chi·ªÅu cao gi·ªØa c√°c l·ªõp",
        "B·ªï sung n·∫πp g√≥c ƒë·ªÉ gi·ªØ th·∫≥ng",
        "ƒê·∫∑t s√°t m√©p trong c·ªßa pallet",
        "Ki·ªÉm tra c√¢n b·∫±ng tr∆∞·ªõc khi r·ªùi v·ªã tr√≠"
    ])
    _prepend_unique(SIMPLE_REC_BANK, "HangHoa", "aisle", [
        "T·∫°o l·ªëi ƒëi r√µ r√†ng gi·ªØa c√°c pallet",
        "Gi·ªØ l·ªëi ƒëi th√¥ng tho√°ng ‚â• 1m",
        "S∆°n/kh√¥i ph·ª•c l·∫°i v·∫°ch an to√†n",
        "Di d·ªùi v·∫≠t c·∫£n kh·ªèi ƒë∆∞·ªùng xe n√¢ng",
        "Lau kh√¥ s√†n, x·ª≠ l√Ω ngay ch·∫•t ƒë·ªï",
        "Kh√¥ng v∆∞·ª£t qua v·∫°ch gi·ªõi h·∫°n"
    ])
    _prepend_unique(SIMPLE_REC_BANK, "HangHoa", "bulky", [
        "ƒêai c·ªë ƒë·ªãnh ch·∫Øc ch·∫Øn c√°c ki·ªán l·ªõn",
        "Th√™m n·∫πp g√≥c/ƒë·ªám b·∫£o v·ªá cho c·∫°nh b√©n",
        "S·∫Øp x·∫øp theo h∆∞·ªõng thu·∫≠n l·ª£i n√¢ng h·∫°",
        "Ghi r√µ k√≠ch th∆∞·ªõc/t·∫£i tr·ªçng tr√™n nh√£n",
        "Ch√®n th√™m ƒëi·ªÉm t·ª±a ch·ªëng x√™ d·ªãch"
    ])

    # ===== KE PALLET =====
    _prepend_unique(SIMPLE_ISSUE_BANK, "KePallet", "align", [
        "Pallet l·ªách m√©p k·ªá",
        "Ki·ªán ch·ªìng qu√° cao m·ª©c cho ph√©p",
        "Thanh gi·∫±ng kh√¥ng c√¢n ƒë·ªëi",
        "Kho·∫£ng c√°ch an to√†n ƒë·ªânh k·ªá kh√¥ng ƒë·ªß"
    ])
    _prepend_unique(SIMPLE_ISSUE_BANK, "KePallet", "tidy", [
        "Pallet h·ªèng ch∆∞a lo·∫°i b·ªè",
        "M·∫£nh g·ªó v·ª•n c√≤n tr√™n s√†n",
        "Tem c≈© c√≤n s√≥t l·∫°i",
        "M√†ng PE d∆∞ ch∆∞a c·∫Øt"
    ])
    _prepend_unique(SIMPLE_REC_BANK, "KePallet", "align", [
        "CƒÉn th·∫≥ng m√©p pallet theo ti√™u chu·∫©n",
        "Kh√¥ng ch·ªìng qu√° quy ƒë·ªãnh chi·ªÅu cao",
        "Ki·ªÉm tra thanh gi·∫±ng v√† c√¢n ch·ªânh l·∫°i",
        "ƒê·∫£m b·∫£o kho·∫£ng c√°ch an to√†n ph·∫ßn ƒë·∫ßu k·ªá"
    ])
    _prepend_unique(SIMPLE_REC_BANK, "KePallet", "tidy", [
        "Lo·∫°i b·ªè pallet h·ªèng ngay",
        "Qu√©t d·ªçn s·∫°ch m·∫£nh g·ªó v·ª•n",
        "B√≥c tem c≈© tr∆∞·ªõc khi d√°n m·ªõi",
        "C·∫Øt g·ªçn m√†ng PE th·ª´a"
    ])

    # ===== LOI DI =====
    _prepend_unique(SIMPLE_ISSUE_BANK, "LoiDi", "aisle", [
        "L·ªëi ƒëi c√≥ v·∫≠t c·∫£n",
        "V·∫°ch d·∫´n h∆∞·ªõng m·ªù/ƒë·ª©t ƒëo·∫°n",
        "H√†ng l·∫•n sang l·ªëi ƒëi",
        "C√≥ ch·∫•t l·ªèng r∆°i v√£i",
        "Thi·∫øu bi·ªÉn h∆∞·ªõng d·∫´n",
        "L·ªëi tho√°t hi·ªÉm ch∆∞a th√¥ng tho√°ng"
    ])
    _prepend_unique(SIMPLE_REC_BANK, "LoiDi", "aisle", [
        "D·ªçn s·∫°ch v·∫≠t c·∫£n ngay",
        "S∆°n l·∫°i v·∫°ch d·∫´n h∆∞·ªõng",
        "S·∫Øp x·∫øp l·∫°i h√†ng v∆∞·ª£t v·∫°ch",
        "Lau s·∫°ch v√† x·ª≠ l√Ω ch·∫•t ƒë·ªï",
        "B·ªï sung bi·ªÉn h∆∞·ªõng d·∫´n r√µ r√†ng",
        "ƒê·∫£m b·∫£o l·ªëi tho√°t hi·ªÉm th√¥ng su·ªët"
    ])

    # ===== VAN PHONG =====
    _prepend_unique(SIMPLE_ISSUE_BANK, "VanPhong", "tidy", [
        "B√†n c√≥ b·ª•i v√† gi·∫•y t·ªù l·ªôn x·ªôn",
        "D·ª•ng c·ª• t·∫£n m√°t, ch∆∞a c√≥ khay",
        "M√†n h√¨nh/b√†n ph√≠m b√°m b·∫©n",
        "D√¢y c√°p r·ªëi d∆∞·ªõi ch√¢n b√†n",
        "Th√πng r√°c ƒë·∫ßy ch∆∞a ƒë·ªï",
        "Nhi·ªÅu v·∫≠t nh·ªè r∆°i v√£i"
    ])
    _prepend_unique(SIMPLE_ISSUE_BANK, "VanPhong", "align", [
        "V·∫≠t d·ª•ng ƒë·∫∑t ch∆∞a ngay ng·∫Øn",
        "T√†i li·ªáu ch∆∞a x·∫øp th·∫≥ng m√©p",
        "M√†n h√¨nh/ƒë·∫ø ƒë·ª° l·ªách"
    ])
    _prepend_unique(SIMPLE_ISSUE_BANK, "VanPhong", "aisle", [
        "L·ªëi ƒëi b·ªã v∆∞·ªõng ƒë·ªì",
        "T√∫i ƒë·ªì ƒë·ªÉ d∆∞·ªõi ch√¢n gh·∫ø"
    ])
    _prepend_unique(SIMPLE_REC_BANK, "VanPhong", "tidy", [
        "Lau b·ª•i b·ªÅ m·∫∑t, kh·ª≠ khu·∫©n",
        "X·∫øp gi·∫•y t·ªù theo nh√≥m/ch·ªß ƒë·ªÅ",
        "D√πng khay/h·ªôp chia √¥ cho d·ª•ng c·ª•",
        "Bu·ªôc g·ªçn d√¢y c√°p s√°t ch√¢n b√†n",
        "ƒê·ªï r√°c ngay khi ƒë·∫ßy"
    ])
    _prepend_unique(SIMPLE_REC_BANK, "VanPhong", "align", [
        "S·∫Øp x·∫øp ƒë·ªì ngay ng·∫Øn, c·ªë ƒë·ªãnh v·ªã tr√≠",
        "CƒÉn th·∫≥ng theo m√©p b√†n/k·ªá"
    ])
    _prepend_unique(SIMPLE_REC_BANK, "VanPhong", "aisle", [
        "D·∫πp ƒë·ªì kh·ªèi l·ªëi ƒëi",
        "Kh√¥ng ƒë·∫∑t t√∫i ƒë·ªì d∆∞·ªõi l·ªëi ch√¢n"
    ])

    # ===== WC =====
    _prepend_unique(SIMPLE_ISSUE_BANK, "WC", "stain", [
        "B·ªÅ m·∫∑t/thi·∫øt b·ªã c√≤n v·∫øt b·∫©n",
        "G∆∞∆°ng v√† tay n·∫Øm ch∆∞a s·∫°ch",
        "V·∫øt ·ªë quanh v√≤i r·ª≠a"
    ])
    _prepend_unique(SIMPLE_ISSUE_BANK, "WC", "trash", [
        "Th√πng r√°c ƒë·∫ßy",
        "R√°c ch∆∞a gom g·ªçn",
        "T√∫i r√°c kh√¥ng thay"
    ])
    _prepend_unique(SIMPLE_ISSUE_BANK, "WC", "dry", [
        "S√†n c√≤n ∆∞·ªõt",
        "C√≥ v·ªát n∆∞·ªõc ƒë·ªçng"
    ])
    _prepend_unique(SIMPLE_REC_BANK, "WC", "stain", [
        "C·ªç r·ª≠a b·∫±ng dung d·ªãch ph√π h·ª£p",
        "Lau s·∫°ch g∆∞∆°ng, tay n·∫Øm",
        "Ch√† s·∫°ch v·∫øt ·ªë quanh v√≤i"
    ])
    _prepend_unique(SIMPLE_REC_BANK, "WC", "trash", [
        "ƒê·ªï r√°c ngay khi ƒë·∫ßy",
        "Thay t√∫i r√°c m·ªõi, d√πng th√πng c√≥ n·∫Øp"
    ])
    _prepend_unique(SIMPLE_REC_BANK, "WC", "dry", [
        "Lau kh√¥ s√†n",
        "ƒê·∫∑t bi·ªÉn c·∫£nh b√°o khi s√†n ∆∞·ªõt"
    ])

try:
    _apply_user_simple_overlay_all()
except Exception:
    pass
# ========= END USER SIMPLE PHRASES =========

